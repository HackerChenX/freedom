# 技术方案思路研讨

[模式: 创新], [AI模型: Gemini 2.5 Pro]

本文档旨在围绕《项目问题诊断与分析报告》中提出的核心问题，进行开放式、多角度的思路探讨。我们将评估不同方法的优缺点，为制定最终的技术执行计划提供基础。

## 1. 关于稳定 `BaseIndicator` 核心接口的思路

报告指出，指标类的接口不统一是导致大量 `AttributeError` 和 `TypeError` 的根源。我们需要一个标准化的接口。

*   **可能性 A：强制接口契约（Strict Contract）**
    *   **想法**：在 `BaseIndicator` 中使用 Python 的 `abc` (Abstract Base Classes) 模块。将 `_calculate`, `calculate_raw_score`, `get_patterns` 等关键方法标记为 `@abstractmethod`。
    *   **优点**：
        *   提供了一个清晰、不容置疑的"模板"。
        *   任何未完整实现接口的子类在实例化时会立刻失败，从而将问题从运行时提前到开发时。
        *   从长远来看，这极大地提升了代码的可维护性和健壮性。
    *   **缺点**：
        *   初期修复工作量较大，需要访问并修改每一个不合规的指标类文件。

*   **可能性 B：温和的默认实现（Gentle Default Implementation）**
    *   **想法**：在 `BaseIndicator` 基类中为这些方法提供一个默认的、可工作的实现。例如，`calculate_raw_score` 可以默认返回一个固定的中性分数（如 0），或者直接 `raise NotImplementedError`。
    *   **优点**：
        *   改动更集中，主要在基类中进行。
        *   可以更快地让测试套件运行起来（即使某些指标的功能不完整），便于我们集中精力修复其他问题。
    *   **缺点**：
        *   可能会隐藏掉部分指标未实现特定功能的事实。开发者可能会忘记在子类中覆盖默认实现。
        *   `raise NotImplementedError` 仍然会将问题推迟到运行时。

*   **考虑因素**：
    *   `set_parameters` 方法是否也应该成为标准接口的一部分？`BOLL` 指标的失败案例暗示了这一点。
    *   我们应该定义一个完整的、权威的指标接口列表，作为本次重构的黄金标准。

## 2. 关于修复或模拟核心组件 (`DataManager`) 的思路

`DataManager` 的功能缺失是导致集成测试大面积失败的关键阻塞点。

*   **可能性 A：实现一个真实的（轻量级）`DataManager`**
    *   **想法**：编写一个能实际工作的 `DataManager`，它可以从一个预定义的 CSV 文件或一个迷你的 SQLite 测试数据库中读取真实的K线数据。
    *   **优点**：
        *   能进行更接近真实的集成测试。
        *   这个组件本身就是项目需要的功能，一举两得。
    *   **缺点**：
        *   前期开发工作量较大。
        *   测试会引入对文件 I/O 或数据库的依赖，可能导致测试变慢或不稳定。

*   **可能性 B：创建一个高保真的模拟（Mock）对象**
    *   **想法**：使用 `unittest.mock` 库创建一个 `MockDataManager`。这个模拟对象将拥有所有需要的方法（如 `get_kline_data`），并可以根据测试需要，被配置为返回特定的、预先定义好的 `pandas.DataFrame` 数据。
    *   **优点**：
        *   将测试与外部数据源完全解耦，使单元测试更快速、更稳定、更可预测。
        *   可以精确控制每个测试用例的输入数据，方便测试边缘情况。
        *   是解除当前测试阻塞的最快方法。
    *   **缺点**：
        *   模拟对象可能与未来 `DataManager` 的真实实现产生偏差。
        *   对于最终的集成测试，我们仍然需要一个真实的 `DataManager`。

*   **考虑因素**：
    *   或许可以采用混合策略：在单元测试（`tests/unit`）中，我们严格使用高保真模拟对象，以确保逻辑的纯粹性；在集成测试（`tests/integration`）中，我们使用真实的 `DataManager` 连接到一个隔离的测试数据库。对于当前阶段，**优先创建 Mock 对象**是最高效的选择。

## 3. 关于清理并修复单元测试的思路

在核心接口和组件稳定后，我们需要系统性地修复单元测试。

*   **可能性 A：逐个击破（File by File）**
    *   **想法**：按照 `pytest` 报告的失败顺序，或者按字母顺序，逐一打开 `tests/unit` 下的测试文件并修复所有失败的测试。
    *   **优点**：
        *   方法简单直接，易于跟踪进度。
    *   **缺点**：
        *   可能会重复解决由同一个根本原因（如 `BaseIndicator` 的问题）导致的多个测试失败，效率不高。

*   **可能性 B：依赖驱动（Dependency-Driven）**
    *   **想法**：这与报告的建议一致。首先稳定 `BaseIndicator` 和 `DataManager` (的 Mock)，然后再去运行和修复依赖它们的测试。
    *   **优点**：
        *   从根源上解决问题，一个核心修复可能会同时让多个测试通过。
        *   逻辑上更清晰，符合软件工程的最佳实践。
    *   **缺点**：
        *   初期可能感觉进展较慢，因为没有立即看到大量测试"变绿"。

*   **考虑因素**：
    *   我们应采纳"依赖驱动"的方法。对于 `test_kdj.py` 中 `KeyError` 这类测试逻辑本身的错误，修复时需要仔细阅读被测试的代码，理解其正确的返回值和数据结构，然后相应地修正测试断言。

## 4. 关于处理 `examples` 目录的思路

`examples` 目录目前是测试噪音的主要来源。

*   **可能性 A：全部修复为测试（Fix as Tests）**
    *   **想法**：投入精力，将 `examples` 目录下的脚本逐一重构为符合规范的、可正常运行的集成测试。这可能包括添加 Mock、重构代码、添加明确的断言等。
    *   **优点**：
        *   可能会挽救一些独特的测试场景，增加整体测试覆盖率。
    *   **缺点**：
        *   根据报告，这些文件问题严重，修复成本可能非常高，会严重拖慢整体进度。

*   **可能性 B：隔离并延后处理（Isolate and Postpone）**
    *   **想法**：立即采取措施将 `examples` 目录从 `pytest` 的测试发现中排除。这可以通过在项目根目录的 `pytest.ini` 或 `pyproject.toml` 配置文件中设置 `norecursedirs = examples` 来实现。
    *   **优点**：
        *   立竿见影，能迅速减少测试报告中的噪音，让我们能专注于 `tests/unit` 和 `tests/integration` 中的核心问题。
        *   将"清理示例"作为一个独立的、优先级较低的任务，可以后续再处理。
    *   **缺点**：
        *   暂时搁置了问题，如果这些示例中包含了重要的业务逻辑测试，可能会被遗漏。

*   **考虑因素**：
    *   "隔离并延后处理"是当前阶段最务实的选择。我们的首要目标是建立一个稳定、可靠的核心测试套件。清理 `examples` 可以在此之后作为"技术债务偿还"的一部分来完成。

## 5. 总体策略思考

*   **暂停新功能**：报告中"暂停新功能开发"的建议是完全正确的。在现有技术债务还清之前，任何新功能都将建立在不稳定的基础之上。
*   **分阶段进行**：将整个修复过程分解为明确的、可管理的阶段是至关重要的。这与我们正在讨论的步骤（稳定接口 -> 修复组件 -> 修复测试）完全一致。

这些思考为我们下一步进入"计划模式"提供了丰富的素材。我们可以在此基础上，选择最合适的路径，并将其细化为具体、可执行的步骤。 