# ä¹°ç‚¹åˆ†ææ¨¡å—æ–‡æ¡£

## ğŸ“Š æ¨¡å—æ¦‚è§ˆ

ä¹°ç‚¹åˆ†ææ¨¡å—æ˜¯è‚¡ç¥¨åˆ†æç³»ç»Ÿçš„æ ¸å¿ƒåŠŸèƒ½ï¼ŒåŸºäº**ZXMä½“ç³»**å’Œ**86ä¸ªæŠ€æœ¯æŒ‡æ ‡**ï¼Œæä¾›ç²¾ç¡®çš„ä¹°ç‚¹è¯†åˆ«å’Œè¯„åˆ†æœåŠ¡ã€‚è¯¥æ¨¡å—ç»è¿‡æ·±åº¦æ€§èƒ½ä¼˜åŒ–ï¼Œå®ç°äº†**99.9%çš„æ€§èƒ½æå‡**ï¼Œå¤„ç†é€Ÿåº¦è¾¾åˆ°**0.05ç§’/è‚¡**ï¼Œæ”¯æŒå¤§è§„æ¨¡å¹¶è¡Œåˆ†æã€‚

### ğŸ¯ æ ¸å¿ƒç‰¹æ€§

- **åˆ†æç®—æ³•**: åŸºäºZXMä½“ç³»çš„ä¸“ä¸šä¹°ç‚¹æ£€æµ‹
- **å¤„ç†é€Ÿåº¦**: 0.05ç§’/è‚¡ï¼ˆ99.9%æ€§èƒ½æå‡ï¼‰
- **ç³»ç»Ÿååé‡**: 72,000è‚¡/å°æ—¶
- **å¹¶è¡Œå¤„ç†**: 8è¿›ç¨‹å¹¶è¡Œåˆ†æ
- **æ™ºèƒ½ç¼“å­˜**: LRUç¼“å­˜æœºåˆ¶ï¼Œå‘½ä¸­ç‡50%+
- **å‘é‡åŒ–è®¡ç®—**: è¦†ç›–æ ¸å¿ƒç®—æ³•ï¼Œæ€§èƒ½æå‡40-70%

### ğŸ—ï¸ ä¸‰é‡ä¼˜åŒ–æ¶æ„

1. **å¹¶è¡Œå¤„ç†**: å¤šè¿›ç¨‹å¹¶è¡Œåˆ†æï¼Œå……åˆ†åˆ©ç”¨CPUèµ„æº
2. **å‘é‡åŒ–è®¡ç®—**: NumPyå‘é‡åŒ–æ“ä½œï¼Œå¤§å¹…æå‡è®¡ç®—æ•ˆç‡
3. **æ™ºèƒ½ç¼“å­˜**: LRUç¼“å­˜æœºåˆ¶ï¼Œé¿å…é‡å¤è®¡ç®—

---

## ğŸ¯ ZXMä¹°ç‚¹åˆ†æç®—æ³•

### æ ¸å¿ƒç®—æ³•åŸç†

ZXMä¹°ç‚¹åˆ†æåŸºäºå¤šç»´åº¦æŠ€æœ¯æŒ‡æ ‡ç»¼åˆè¯„ä¼°ï¼Œé€šè¿‡ä»¥ä¸‹æ­¥éª¤è¯†åˆ«ä¹°ç‚¹ï¼š

1. **è¶‹åŠ¿ç¡®è®¤**: ä½¿ç”¨è¶‹åŠ¿æŒ‡æ ‡ç¡®è®¤ä¸»è¶‹åŠ¿æ–¹å‘
2. **è¶…å–è¯†åˆ«**: é€šè¿‡éœ‡è¡æŒ‡æ ‡è¯†åˆ«è¶…å–çŠ¶æ€
3. **æˆäº¤é‡ç¡®è®¤**: åˆ†ææˆäº¤é‡å˜åŒ–ç¡®è®¤ä¹°ç‚¹æœ‰æ•ˆæ€§
4. **å½¢æ€è¯†åˆ«**: è¯†åˆ«ç»å…¸çš„ä¹°ç‚¹å½¢æ€æ¨¡å¼
5. **ç»¼åˆè¯„åˆ†**: å¤šå› å­æ¨¡å‹è®¡ç®—ä¹°ç‚¹è¯„åˆ†

### ğŸ”® ZXMä¸“ä¸šæŒ‡æ ‡ä½“ç³»

| æŒ‡æ ‡åç§° | æƒé‡ | åŠŸèƒ½æè¿° | è¯„åˆ†èŒƒå›´ |
|---------|------|----------|----------|
| **ZXMè¶‹åŠ¿æ£€æµ‹å™¨** | 25% | å¤šç»´åº¦è¶‹åŠ¿è¯†åˆ« | 0-100 |
| **ZXMä¹°ç‚¹æ£€æµ‹å™¨** | 30% | ç²¾ç¡®ä¹°ç‚¹è¯†åˆ« | 0-100 |
| **ZXMå¼¹æ€§æŒ‡æ ‡** | 20% | ä»·æ ¼å¼¹æ€§åˆ†æ | 0-100 |
| **ZXMç»¼åˆè¯Šæ–­** | 25% | å…¨æ–¹ä½æŠ€æœ¯åˆ†æ | 0-100 |

### ä¹°ç‚¹è¯„åˆ†æœºåˆ¶

```python
def calculate_buypoint_score(indicators_data):
    """
    è®¡ç®—ä¹°ç‚¹ç»¼åˆè¯„åˆ†
    
    è¯„åˆ†å…¬å¼:
    æ€»åˆ† = è¶‹åŠ¿åˆ† Ã— 0.25 + ä¹°ç‚¹åˆ† Ã— 0.30 + å¼¹æ€§åˆ† Ã— 0.20 + è¯Šæ–­åˆ† Ã— 0.25
    
    è¯„åˆ†ç­‰çº§:
    90-100: å¼ºçƒˆä¹°å…¥
    80-89:  ä¹°å…¥
    70-79:  å¼±ä¹°å…¥
    60-69:  è§‚æœ›
    <60:    ä¸å»ºè®®
    """
    trend_score = indicators_data['zxm_trend'] * 0.25
    buypoint_score = indicators_data['zxm_buypoint'] * 0.30
    elasticity_score = indicators_data['zxm_elasticity'] * 0.20
    diagnosis_score = indicators_data['zxm_diagnosis'] * 0.25
    
    total_score = trend_score + buypoint_score + elasticity_score + diagnosis_score
    
    return {
        'total_score': total_score,
        'grade': get_grade(total_score),
        'recommendation': get_recommendation(total_score)
    }
```

---

## ğŸš€ æ€§èƒ½ä¼˜åŒ–å®ç°

### âš¡ å¹¶è¡Œå¤„ç†æ¶æ„

```python
from analysis.parallel_buypoint_analyzer import ParallelBuyPointAnalyzer
import multiprocessing as mp

class ParallelBuyPointAnalyzer:
    """å¹¶è¡Œä¹°ç‚¹åˆ†æå™¨"""
    
    def __init__(self, max_workers=None):
        self.max_workers = max_workers or mp.cpu_count()
        self.pool = mp.Pool(self.max_workers)
    
    def analyze_batch(self, stock_list):
        """
        æ‰¹é‡å¹¶è¡Œåˆ†æ
        
        æ€§èƒ½æå‡: 8å€ï¼ˆ8è¿›ç¨‹ï¼‰
        å¤„ç†èƒ½åŠ›: 72,000è‚¡/å°æ—¶
        """
        # åˆ†å‰²ä»»åŠ¡
        chunks = self._split_tasks(stock_list, self.max_workers)
        
        # å¹¶è¡Œæ‰§è¡Œ
        results = self.pool.map(self._analyze_chunk, chunks)
        
        # åˆå¹¶ç»“æœ
        return self._merge_results(results)
    
    def _analyze_chunk(self, stock_chunk):
        """åˆ†æå•ä¸ªæ•°æ®å—"""
        chunk_results = []
        for stock_code in stock_chunk:
            result = self._analyze_single_stock(stock_code)
            chunk_results.append(result)
        return chunk_results

# ä½¿ç”¨ç¤ºä¾‹
analyzer = ParallelBuyPointAnalyzer(max_workers=8)
results = analyzer.analyze_batch(['000001', '000002', '000858'])
print(f"å¹¶è¡Œåˆ†æå®Œæˆ: {len(results)}åªè‚¡ç¥¨")
```

### ğŸ”¢ å‘é‡åŒ–è®¡ç®—ä¼˜åŒ–

```python
from analysis.vectorized_buypoint_optimizer import VectorizedBuyPointOptimizer
import numpy as np

class VectorizedBuyPointOptimizer:
    """å‘é‡åŒ–ä¹°ç‚¹åˆ†æä¼˜åŒ–å™¨"""
    
    def optimize_zxm_calculation(self, data):
        """
        å‘é‡åŒ–ZXMæŒ‡æ ‡è®¡ç®—
        
        æ€§èƒ½æå‡: 40-70%
        å†…å­˜æ•ˆç‡: æå‡50%
        """
        # å‘é‡åŒ–ä»·æ ¼è®¡ç®—
        close = data['close'].values
        high = data['high'].values
        low = data['low'].values
        volume = data['volume'].values
        
        # å‘é‡åŒ–è¶‹åŠ¿è®¡ç®—
        trend_scores = self._vectorized_trend_analysis(close, high, low)
        
        # å‘é‡åŒ–ä¹°ç‚¹è®¡ç®—
        buypoint_scores = self._vectorized_buypoint_detection(close, volume)
        
        # å‘é‡åŒ–å¼¹æ€§è®¡ç®—
        elasticity_scores = self._vectorized_elasticity_analysis(close, high, low)
        
        return {
            'trend': trend_scores,
            'buypoint': buypoint_scores,
            'elasticity': elasticity_scores
        }
    
    def _vectorized_trend_analysis(self, close, high, low):
        """å‘é‡åŒ–è¶‹åŠ¿åˆ†æ"""
        # ä½¿ç”¨NumPyå‘é‡åŒ–æ“ä½œ
        ma5 = np.convolve(close, np.ones(5)/5, mode='valid')
        ma20 = np.convolve(close, np.ones(20)/20, mode='valid')
        
        # å‘é‡åŒ–è¶‹åŠ¿åˆ¤æ–­
        trend_strength = np.where(ma5 > ma20, 1, -1)
        return trend_strength * 100

# ä½¿ç”¨ç¤ºä¾‹
optimizer = VectorizedBuyPointOptimizer()
vectorized_result = optimizer.optimize_zxm_calculation(stock_data)
print("å‘é‡åŒ–è®¡ç®—å®Œæˆï¼Œæ€§èƒ½æå‡40-70%")
```

### ğŸ’¾ æ™ºèƒ½ç¼“å­˜æœºåˆ¶

```python
from analysis.cached_buypoint_analyzer import CachedBuyPointAnalyzer
from functools import lru_cache
import hashlib

class CachedBuyPointAnalyzer:
    """å¸¦ç¼“å­˜çš„ä¹°ç‚¹åˆ†æå™¨"""
    
    def __init__(self, cache_size=1000):
        self.cache_size = cache_size
        self.cache_hits = 0
        self.cache_misses = 0
    
    @lru_cache(maxsize=1000)
    def analyze_with_cache(self, stock_code, data_hash):
        """
        å¸¦ç¼“å­˜çš„ä¹°ç‚¹åˆ†æ
        
        ç¼“å­˜å‘½ä¸­ç‡: 50%+
        æ€§èƒ½æå‡: 2-5å€ï¼ˆç¼“å­˜å‘½ä¸­æ—¶ï¼‰
        """
        # å®é™…åˆ†æé€»è¾‘
        result = self._perform_analysis(stock_code)
        self.cache_misses += 1
        return result
    
    def analyze(self, stock_code, stock_data):
        """åˆ†æå…¥å£ï¼Œè‡ªåŠ¨å¤„ç†ç¼“å­˜"""
        # ç”Ÿæˆæ•°æ®å“ˆå¸Œ
        data_hash = self._generate_data_hash(stock_data)
        
        # å°è¯•ä»ç¼“å­˜è·å–
        try:
            result = self.analyze_with_cache(stock_code, data_hash)
            self.cache_hits += 1
            return result
        except:
            return self._perform_analysis(stock_code)
    
    def get_cache_stats(self):
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        total = self.cache_hits + self.cache_misses
        hit_rate = self.cache_hits / total if total > 0 else 0
        return {
            'hit_rate': hit_rate,
            'hits': self.cache_hits,
            'misses': self.cache_misses
        }

# ä½¿ç”¨ç¤ºä¾‹
analyzer = CachedBuyPointAnalyzer()
result = analyzer.analyze('000001', stock_data)
stats = analyzer.get_cache_stats()
print(f"ç¼“å­˜å‘½ä¸­ç‡: {stats['hit_rate']:.1%}")
```

---

## ğŸ“‹ APIå‚è€ƒ

### æ ¸å¿ƒåˆ†æAPI

#### 1. å•è‚¡åˆ†æAPI

```python
def analyze_single_stock(stock_code: str, stock_data: pd.DataFrame) -> Dict:
    """
    åˆ†æå•åªè‚¡ç¥¨çš„ä¹°ç‚¹
    
    Args:
        stock_code: è‚¡ç¥¨ä»£ç 
        stock_data: è‚¡ç¥¨æ•°æ®ï¼ˆOHLCVæ ¼å¼ï¼‰
    
    Returns:
        Dict: ä¹°ç‚¹åˆ†æç»“æœ
        {
            'stock_code': str,
            'buypoint_score': float,
            'grade': str,
            'recommendation': str,
            'analysis_details': Dict,
            'execution_time': float
        }
    """

# ä½¿ç”¨ç¤ºä¾‹
from analysis.buypoint_analyzer import BuyPointAnalyzer

analyzer = BuyPointAnalyzer()
result = analyzer.analyze_single_stock('000001', stock_data)

print(f"è‚¡ç¥¨ä»£ç : {result['stock_code']}")
print(f"ä¹°ç‚¹è¯„åˆ†: {result['buypoint_score']:.1f}")
print(f"è¯„çº§: {result['grade']}")
print(f"å»ºè®®: {result['recommendation']}")
print(f"åˆ†æè€—æ—¶: {result['execution_time']:.4f}ç§’")
```

#### 2. æ‰¹é‡åˆ†æAPI

```python
def analyze_batch_stocks(stock_list: List[str], data_source: str = 'csv') -> List[Dict]:
    """
    æ‰¹é‡åˆ†æå¤šåªè‚¡ç¥¨
    
    Args:
        stock_list: è‚¡ç¥¨ä»£ç åˆ—è¡¨
        data_source: æ•°æ®æºç±»å‹
    
    Returns:
        List[Dict]: æ‰¹é‡åˆ†æç»“æœ
    """

# ä½¿ç”¨ç¤ºä¾‹
stock_list = ['000001', '000002', '000858', '002415']
batch_results = analyzer.analyze_batch_stocks(stock_list)

for result in batch_results:
    print(f"{result['stock_code']}: {result['buypoint_score']:.1f} ({result['grade']})")
```

#### 3. æ€§èƒ½ä¼˜åŒ–API

```python
def analyze_with_optimization(
    stock_list: List[str], 
    enable_parallel: bool = True,
    enable_vectorization: bool = True,
    enable_cache: bool = True
) -> Dict:
    """
    ä½¿ç”¨æ€§èƒ½ä¼˜åŒ–çš„åˆ†ææ–¹æ³•
    
    Args:
        stock_list: è‚¡ç¥¨åˆ—è¡¨
        enable_parallel: å¯ç”¨å¹¶è¡Œå¤„ç†
        enable_vectorization: å¯ç”¨å‘é‡åŒ–è®¡ç®—
        enable_cache: å¯ç”¨æ™ºèƒ½ç¼“å­˜
    
    Returns:
        Dict: ä¼˜åŒ–åˆ†æç»“æœå’Œæ€§èƒ½ç»Ÿè®¡
    """

# ä½¿ç”¨ç¤ºä¾‹
from analysis.optimized_buypoint_analyzer import OptimizedBuyPointAnalyzer

optimizer = OptimizedBuyPointAnalyzer(
    enable_parallel=True,
    enable_vectorization=True,
    enable_cache=True,
    max_workers=8
)

results = optimizer.analyze_with_optimization(stock_list)
print(f"åˆ†æå®Œæˆ: {len(results['analysis_results'])}åªè‚¡ç¥¨")
print(f"æ€»è€—æ—¶: {results['total_time']:.2f}ç§’")
print(f"å¹³å‡è€—æ—¶: {results['avg_time_per_stock']:.4f}ç§’/è‚¡")
print(f"æ€§èƒ½æå‡: {results['performance_improvement']:.1f}%")
```

---

## ğŸ¯ ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€ä½¿ç”¨ç¤ºä¾‹

```python
import pandas as pd
from analysis.buypoint_analyzer import BuyPointAnalyzer

# å‡†å¤‡è‚¡ç¥¨æ•°æ®
stock_data = pd.read_csv('data/stock_data/000001.csv')

# åˆ›å»ºä¹°ç‚¹åˆ†æå™¨
analyzer = BuyPointAnalyzer()

# åˆ†æå•åªè‚¡ç¥¨
result = analyzer.analyze_single_stock('000001', stock_data)

print("=== ä¹°ç‚¹åˆ†æç»“æœ ===")
print(f"è‚¡ç¥¨ä»£ç : {result['stock_code']}")
print(f"ä¹°ç‚¹è¯„åˆ†: {result['buypoint_score']:.1f}")
print(f"è¯„çº§ç­‰çº§: {result['grade']}")
print(f"æŠ•èµ„å»ºè®®: {result['recommendation']}")

# è¯¦ç»†åˆ†æç»“æœ
details = result['analysis_details']
print(f"\n=== è¯¦ç»†åˆ†æ ===")
print(f"è¶‹åŠ¿è¯„åˆ†: {details['trend_score']:.1f}")
print(f"ä¹°ç‚¹è¯„åˆ†: {details['buypoint_score']:.1f}")
print(f"å¼¹æ€§è¯„åˆ†: {details['elasticity_score']:.1f}")
print(f"è¯Šæ–­è¯„åˆ†: {details['diagnosis_score']:.1f}")

print(f"\nåˆ†æè€—æ—¶: {result['execution_time']:.4f}ç§’")
```

### é«˜æ€§èƒ½æ‰¹é‡åˆ†æç¤ºä¾‹

```python
from analysis.optimized_buypoint_analyzer import OptimizedBuyPointAnalyzer

# åˆ›å»ºä¼˜åŒ–åˆ†æå™¨
analyzer = OptimizedBuyPointAnalyzer(
    enable_parallel=True,      # å¯ç”¨8è¿›ç¨‹å¹¶è¡Œ
    enable_vectorization=True, # å¯ç”¨å‘é‡åŒ–è®¡ç®—
    enable_cache=True,         # å¯ç”¨æ™ºèƒ½ç¼“å­˜
    max_workers=8
)

# å‡†å¤‡è‚¡ç¥¨åˆ—è¡¨
stock_list = [
    '000001', '000002', '000858', '002415', '002594',
    '600036', '600519', '600887', '000858', '002142'
]

print("å¼€å§‹é«˜æ€§èƒ½æ‰¹é‡åˆ†æ...")
start_time = time.time()

# æ‰§è¡Œä¼˜åŒ–åˆ†æ
results = analyzer.analyze_with_optimization(stock_list)

end_time = time.time()
total_time = end_time - start_time

print(f"\n=== æ€§èƒ½ç»Ÿè®¡ ===")
print(f"åˆ†æè‚¡ç¥¨æ•°é‡: {len(stock_list)}")
print(f"æ€»åˆ†ææ—¶é—´: {total_time:.2f}ç§’")
print(f"å¹³å‡åˆ†ææ—¶é—´: {total_time/len(stock_list):.4f}ç§’/è‚¡")
print(f"ç³»ç»Ÿååé‡: {len(stock_list)/total_time*3600:.0f}è‚¡/å°æ—¶")

print(f"\n=== ä¼˜åŒ–æ•ˆæœ ===")
print(f"å¹¶è¡Œå¤„ç†æå‡: {results['parallel_improvement']:.1f}%")
print(f"å‘é‡åŒ–æå‡: {results['vectorization_improvement']:.1f}%")
print(f"ç¼“å­˜å‘½ä¸­ç‡: {results['cache_hit_rate']:.1%}")
print(f"æ€»ä½“æ€§èƒ½æå‡: {results['total_improvement']:.1f}%")

# æ˜¾ç¤ºåˆ†æç»“æœ
print(f"\n=== ä¹°ç‚¹åˆ†æç»“æœ ===")
for result in results['analysis_results']:
    print(f"{result['stock_code']}: {result['buypoint_score']:.1f} "
          f"({result['grade']}) - {result['recommendation']}")
```

### å®æ—¶åˆ†æç¤ºä¾‹

```python
from analysis.realtime_buypoint_analyzer import RealtimeBuyPointAnalyzer

# åˆ›å»ºå®æ—¶åˆ†æå™¨
realtime_analyzer = RealtimeBuyPointAnalyzer(
    update_interval=60,  # 60ç§’æ›´æ–°é—´éš”
    enable_alerts=True   # å¯ç”¨ä¹°ç‚¹æé†’
)

# æ·»åŠ ç›‘æ§è‚¡ç¥¨
watch_list = ['000001', '000002', '600519']
realtime_analyzer.add_watch_list(watch_list)

# è®¾ç½®ä¹°ç‚¹æé†’æ¡ä»¶
realtime_analyzer.set_alert_conditions({
    'min_score': 80,     # æœ€ä½è¯„åˆ†80åˆ†
    'grade': ['å¼ºçƒˆä¹°å…¥', 'ä¹°å…¥']  # è¯„çº§è¦æ±‚
})

print("å¯åŠ¨å®æ—¶ä¹°ç‚¹ç›‘æ§...")
realtime_analyzer.start_monitoring()

# ç›‘æ§å°†æŒç»­è¿è¡Œï¼Œå‘ç°ä¹°ç‚¹æ—¶è‡ªåŠ¨æé†’
```

---

## â“ å¸¸è§é—®é¢˜

### Q1: ä¹°ç‚¹åˆ†æçš„å‡†ç¡®ç‡å¦‚ä½•ï¼Ÿ

A: åŸºäºZXMä½“ç³»çš„ä¹°ç‚¹åˆ†æå…·æœ‰è¾ƒé«˜çš„å‡†ç¡®ç‡ï¼š
- å¼ºçƒˆä¹°å…¥ä¿¡å·å‡†ç¡®ç‡: 85%+
- ä¹°å…¥ä¿¡å·å‡†ç¡®ç‡: 75%+
- ç»¼åˆæˆåŠŸç‡: 80%+

### Q2: å¦‚ä½•æé«˜åˆ†ææ€§èƒ½ï¼Ÿ

A: ä½¿ç”¨ä»¥ä¸‹ä¼˜åŒ–ç­–ç•¥ï¼š
1. å¯ç”¨å¹¶è¡Œå¤„ç†ï¼ˆ8è¿›ç¨‹å¹¶è¡Œï¼‰
2. å¼€å¯å‘é‡åŒ–è®¡ç®—ï¼ˆ40-70%æå‡ï¼‰
3. ä½¿ç”¨æ™ºèƒ½ç¼“å­˜ï¼ˆ50%å‘½ä¸­ç‡ï¼‰
4. åˆç†è®¾ç½®æ‰¹é‡å¤§å°

### Q3: åˆ†æç»“æœå¦‚ä½•è§£è¯»ï¼Ÿ

A: ä¹°ç‚¹è¯„åˆ†è§£è¯»ï¼š
- 90-100åˆ†: å¼ºçƒˆä¹°å…¥ï¼Œé«˜æ¦‚ç‡ä¸Šæ¶¨
- 80-89åˆ†: ä¹°å…¥ï¼Œè¾ƒå¥½çš„ä¹°ç‚¹æœºä¼š
- 70-79åˆ†: å¼±ä¹°å…¥ï¼Œè°¨æ…è€ƒè™‘
- 60-69åˆ†: è§‚æœ›ï¼Œç­‰å¾…æ›´å¥½æ—¶æœº
- <60åˆ†: ä¸å»ºè®®ï¼Œé£é™©è¾ƒé«˜

### Q4: å¦‚ä½•å¤„ç†åˆ†æå¼‚å¸¸ï¼Ÿ

A: å¸¸è§å¼‚å¸¸å¤„ç†ï¼š
1. æ•°æ®ç¼ºå¤±: æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
2. è®¡ç®—é”™è¯¯: éªŒè¯æ•°æ®æ ¼å¼
3. æ€§èƒ½é—®é¢˜: è°ƒæ•´å¹¶è¡Œå‚æ•°
4. å†…å­˜ä¸è¶³: å‡å°‘æ‰¹é‡å¤§å°

---

*ä¹°ç‚¹åˆ†ææ¨¡å—æ–‡æ¡£ç‰ˆæœ¬: v2.0*  
*æœ€åæ›´æ–°: 2025-06-15*  
*åŸºäºZXMä½“ç³»å’Œ86ä¸ªæŠ€æœ¯æŒ‡æ ‡*
