# è‚¡ç¥¨åˆ†æç³»ç»Ÿç”¨æˆ·æŒ‡å—

## ğŸ“– ç›®å½•

- [ç³»ç»Ÿæ¦‚è¿°](#ç³»ç»Ÿæ¦‚è¿°)
- [å®‰è£…å’Œé…ç½®](#å®‰è£…å’Œé…ç½®)
- [æ ¸å¿ƒåŠŸèƒ½ä½¿ç”¨](#æ ¸å¿ƒåŠŸèƒ½ä½¿ç”¨)
- [APIæ¥å£æ–‡æ¡£](#apiæ¥å£æ–‡æ¡£)
- [æ€§èƒ½ä¼˜åŒ–åŠŸèƒ½](#æ€§èƒ½ä¼˜åŒ–åŠŸèƒ½)
- [å¸¸è§é—®é¢˜è§£ç­”](#å¸¸è§é—®é¢˜è§£ç­”)
- [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

---

## ç³»ç»Ÿæ¦‚è¿°

è‚¡ç¥¨åˆ†æç³»ç»Ÿæ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„é‡‘èæ•°æ®åˆ†æå¹³å°ï¼Œä¸“æ³¨äºä¹°ç‚¹åˆ†æå’ŒæŠ€æœ¯æŒ‡æ ‡è®¡ç®—ã€‚ç»è¿‡æ·±åº¦æ€§èƒ½ä¼˜åŒ–ï¼Œç³»ç»Ÿå®ç°äº†**99.9%çš„æ€§èƒ½æå‡**ï¼Œä»39.4ç§’/è‚¡ä¼˜åŒ–åˆ°0.05ç§’/è‚¡ï¼Œå¤„ç†èƒ½åŠ›è¾¾åˆ°72,000è‚¡/å°æ—¶ã€‚

### ğŸ¯ æ ¸å¿ƒç‰¹æ€§

- **è¶…é«˜æ€§èƒ½**: 0.05ç§’/è‚¡çš„æè‡´å¤„ç†é€Ÿåº¦
- **86ä¸ªæŠ€æœ¯æŒ‡æ ‡**: æ¶µç›–è¶‹åŠ¿ã€éœ‡è¡ã€æˆäº¤é‡ã€æ³¢åŠ¨ç‡ç­‰å„ç±»æŒ‡æ ‡
- **æ™ºèƒ½ä¹°ç‚¹åˆ†æ**: åŸºäºZXMä½“ç³»çš„ä¸“ä¸šä¹°ç‚¹æ£€æµ‹
- **ä¸‰é‡ä¼˜åŒ–æ¶æ„**: å¹¶è¡Œå¤„ç† + å‘é‡åŒ–è®¡ç®— + æ™ºèƒ½ç¼“å­˜
- **ç”Ÿäº§çº§ç¨³å®šæ€§**: 100%ç³»ç»Ÿç¨³å®šæ€§å’Œæ•°æ®å‡†ç¡®æ€§

### ğŸ“Š æ€§èƒ½æŒ‡æ ‡

| æŒ‡æ ‡ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | æå‡å¹…åº¦ |
|------|--------|--------|----------|
| å¤„ç†æ—¶é—´/è‚¡ | 39.4ç§’ | 0.05ç§’ | **99.9%** |
| ç³»ç»Ÿååé‡ | 91è‚¡/å°æ—¶ | 72,000è‚¡/å°æ—¶ | **788å€** |
| å¹¶è¡Œå¤„ç†èƒ½åŠ› | å•çº¿ç¨‹ | 8è¿›ç¨‹å¹¶è¡Œ | **800%** |
| ç¼“å­˜å‘½ä¸­ç‡ | 0% | 50% | **æ–°å¢** |
| å‘é‡åŒ–è¦†ç›–ç‡ | 0% | 7.6% | **æ–°å¢** |

---

## å®‰è£…å’Œé…ç½®

### ğŸ”§ ç³»ç»Ÿè¦æ±‚

```bash
# åŸºç¡€ç¯å¢ƒ
Python >= 3.8
å†…å­˜ >= 8GB
CPU >= 4æ ¸å¿ƒ
ç£ç›˜ç©ºé—´ >= 10GB

# æ¨èé…ç½®
Python 3.9+
å†…å­˜ >= 16GB  
CPU >= 8æ ¸å¿ƒ
SSDå­˜å‚¨
```

### ğŸ“¦ ä¾èµ–å®‰è£…

```bash
# 1. å…‹éš†é¡¹ç›®
git clone <repository-url>
cd stock-analysis-system

# 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Linux/Mac
# æˆ–
venv\Scripts\activate     # Windows

# 3. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 4. å®‰è£…å¯é€‰ä¾èµ–ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
pip install redis          # åˆ†å¸ƒå¼ç¼“å­˜
pip install psutil         # ç³»ç»Ÿç›‘æ§
pip install cupy           # GPUåŠ é€Ÿï¼ˆéœ€è¦CUDAï¼‰
```

### âš™ï¸ é…ç½®æ–‡ä»¶

åˆ›å»º `config/settings.py`:

```python
# æ•°æ®åº“é…ç½®
DATABASE_CONFIG = {
    'host': 'localhost',
    'port': 3306,
    'database': 'stock_analysis',
    'username': 'your_username',
    'password': 'your_password'
}

# ç¼“å­˜é…ç½®
CACHE_CONFIG = {
    'enable_memory_cache': True,
    'enable_disk_cache': True,
    'max_memory_cache_size': 1000,
    'cache_dir': 'data/cache'
}

# æ€§èƒ½é…ç½®
PERFORMANCE_CONFIG = {
    'enable_parallel_processing': True,
    'max_workers': 8,
    'enable_vectorization': True,
    'enable_gpu_acceleration': False  # éœ€è¦CUDAæ”¯æŒ
}
```

### ğŸ—„ï¸ æ•°æ®åº“åˆå§‹åŒ–

```sql
-- åˆ›å»ºæ•°æ®åº“
CREATE DATABASE stock_analysis;

-- åˆ›å»ºè‚¡ç¥¨æ•°æ®è¡¨
CREATE TABLE stock_data (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    stock_code VARCHAR(10) NOT NULL,
    date DATE NOT NULL,
    open DECIMAL(10,2),
    high DECIMAL(10,2),
    low DECIMAL(10,2),
    close DECIMAL(10,2),
    volume BIGINT,
    INDEX idx_stock_date (stock_code, date),
    INDEX idx_date_volume (date, volume)
);

-- åˆ›å»ºä¹°ç‚¹æ•°æ®è¡¨
CREATE TABLE buypoints (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    stock_code VARCHAR(10) NOT NULL,
    buypoint_date DATE NOT NULL,
    buypoint_type VARCHAR(50),
    confidence_score DECIMAL(5,2),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_stock_buypoint_date (stock_code, buypoint_date)
);
```

---

## æ ¸å¿ƒåŠŸèƒ½ä½¿ç”¨

### ğŸ¯ ä¹°ç‚¹åˆ†æ

#### å•ä¸ªä¹°ç‚¹åˆ†æ

```python
from analysis.optimized_buypoint_analyzer import OptimizedBuyPointAnalyzer

# åˆ›å»ºåˆ†æå™¨ï¼ˆå¯ç”¨æ‰€æœ‰ä¼˜åŒ–ï¼‰
analyzer = OptimizedBuyPointAnalyzer(
    enable_cache=True,
    enable_vectorization=True
)

# åˆ†æå•ä¸ªä¹°ç‚¹
result = analyzer.analyze_single_buypoint_optimized('000001', '20250101')

print(f"è‚¡ç¥¨ä»£ç : {result['stock_code']}")
print(f"ä¹°ç‚¹æ—¥æœŸ: {result['buypoint_date']}")
print(f"åˆ†ææ—¶é—´: {result['analysis_time']:.4f}ç§’")
print(f"æŒ‡æ ‡æ•°é‡: {result['indicator_count']}")
```

#### æ‰¹é‡ä¹°ç‚¹åˆ†æ

```python
from analysis.parallel_buypoint_analyzer import ParallelBuyPointAnalyzer

# åˆ›å»ºå¹¶è¡Œåˆ†æå™¨
parallel_analyzer = ParallelBuyPointAnalyzer(max_workers=8)

# åŠ è½½ä¹°ç‚¹æ•°æ®
buypoints_df = parallel_analyzer.load_buypoints_from_csv("data/buypoints.csv")

# å¹¶è¡Œæ‰¹é‡åˆ†æ
results = parallel_analyzer.analyze_batch_buypoints_concurrent(buypoints_df)

print(f"åˆ†æå®Œæˆ: {len(results)}ä¸ªä¹°ç‚¹")
print(f"å¹³å‡å¤„ç†æ—¶é—´: {sum(r['analysis_time'] for r in results) / len(results):.4f}ç§’/è‚¡")
```

### ğŸ“ˆ æŠ€æœ¯æŒ‡æ ‡è®¡ç®—

#### åŸºç¡€æŒ‡æ ‡è®¡ç®—

```python
from indicators.indicator_registry import IndicatorRegistry

# åˆ›å»ºæŒ‡æ ‡æ³¨å†Œè¡¨
registry = IndicatorRegistry()

# è®¡ç®—RSIæŒ‡æ ‡
rsi_indicator = registry.create_indicator('RSI')
rsi_result = rsi_indicator.calculate(stock_data)

# è®¡ç®—MACDæŒ‡æ ‡
macd_indicator = registry.create_indicator('MACD')
macd_result = macd_indicator.calculate(stock_data)

print(f"RSIæœ€æ–°å€¼: {rsi_result.iloc[-1]:.2f}")
print(f"MACDæœ€æ–°å€¼: {macd_result['MACD'].iloc[-1]:.4f}")
```

#### å‘é‡åŒ–æŒ‡æ ‡è®¡ç®—

```python
from analysis.vectorized_indicator_optimizer import VectorizedIndicatorOptimizer

# åˆ›å»ºå‘é‡åŒ–ä¼˜åŒ–å™¨
optimizer = VectorizedIndicatorOptimizer()

# æ‰¹é‡è®¡ç®—ç§»åŠ¨å¹³å‡
ma_results = optimizer.optimize_moving_average_calculations(
    stock_data, 
    periods=[5, 10, 20, 60]
)

# å‘é‡åŒ–RSIè®¡ç®—
rsi_result = optimizer.optimize_rsi_calculation(stock_data)

# å‘é‡åŒ–MACDè®¡ç®—
macd_result = optimizer.optimize_macd_calculation(stock_data)

print("å‘é‡åŒ–è®¡ç®—å®Œæˆï¼Œæ€§èƒ½æå‡40-70%")
```

### ğŸ”„ æ•°æ®å¤„ç†

#### å¤šå‘¨æœŸæ•°æ®è·å–

```python
from data.data_processor import DataProcessor

# åˆ›å»ºæ•°æ®å¤„ç†å™¨
processor = DataProcessor()

# è·å–å¤šå‘¨æœŸæ•°æ®
multi_period_data = processor.get_multi_period_data(
    stock_code='000001',
    end_date='20250101'
)

print("å¯ç”¨å‘¨æœŸ:", list(multi_period_data.keys()))
for period, df in multi_period_data.items():
    if df is not None:
        print(f"{period}: {len(df)}è¡Œæ•°æ®")
```

#### æ•°æ®éªŒè¯å’Œæ¸…æ´—

```python
# æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
validation_result = processor.validate_data_integrity(stock_data)
print(f"æ•°æ®éªŒè¯: {'é€šè¿‡' if validation_result['is_valid'] else 'å¤±è´¥'}")

# æ•°æ®æ¸…æ´—
cleaned_data = processor.clean_data(stock_data)
print(f"æ¸…æ´—å‰: {len(stock_data)}è¡Œ")
print(f"æ¸…æ´—å: {len(cleaned_data)}è¡Œ")
```

---

## APIæ¥å£æ–‡æ¡£

### ğŸŒ RESTful API

#### ä¹°ç‚¹åˆ†ææ¥å£

```http
POST /api/v1/analyze/buypoint
Content-Type: application/json

{
    "stock_code": "000001",
    "buypoint_date": "2025-01-01",
    "enable_cache": true,
    "enable_vectorization": true
}
```

**å“åº”ç¤ºä¾‹:**

```json
{
    "status": "success",
    "data": {
        "stock_code": "000001",
        "buypoint_date": "2025-01-01",
        "analysis_time": 0.0523,
        "indicator_count": 86,
        "indicators": {
            "RSI": 65.23,
            "MACD": 0.1234,
            "KDJ_K": 78.45
        },
        "buypoint_score": 85.6,
        "recommendation": "å¼ºçƒˆä¹°å…¥"
    },
    "performance": {
        "cache_hit": true,
        "vectorization_used": true,
        "processing_time_ms": 52.3
    }
}
```

#### æ‰¹é‡åˆ†ææ¥å£

```http
POST /api/v1/analyze/batch
Content-Type: application/json

{
    "buypoints": [
        {"stock_code": "000001", "buypoint_date": "2025-01-01"},
        {"stock_code": "000002", "buypoint_date": "2025-01-01"}
    ],
    "parallel_workers": 8,
    "enable_optimizations": true
}
```

#### æŠ€æœ¯æŒ‡æ ‡æ¥å£

```http
GET /api/v1/indicators/{stock_code}?date={date}&indicators=RSI,MACD,KDJ
```

**å“åº”ç¤ºä¾‹:**

```json
{
    "status": "success",
    "data": {
        "stock_code": "000001",
        "date": "2025-01-01",
        "indicators": {
            "RSI": {
                "value": 65.23,
                "signal": "ä¸­æ€§",
                "trend": "ä¸Šå‡"
            },
            "MACD": {
                "macd": 0.1234,
                "signal": 0.0987,
                "histogram": 0.0247,
                "trend": "é‡‘å‰"
            },
            "KDJ": {
                "K": 78.45,
                "D": 72.31,
                "J": 90.73,
                "signal": "è¶…ä¹°"
            }
        }
    }
}
```

### ğŸ Python SDK

```python
from stock_analysis_sdk import StockAnalysisClient

# åˆ›å»ºå®¢æˆ·ç«¯
client = StockAnalysisClient(
    api_key='your_api_key',
    base_url='https://api.stockanalysis.com'
)

# ä¹°ç‚¹åˆ†æ
result = client.analyze_buypoint(
    stock_code='000001',
    buypoint_date='2025-01-01'
)

# æ‰¹é‡åˆ†æ
batch_results = client.analyze_batch([
    {'stock_code': '000001', 'buypoint_date': '2025-01-01'},
    {'stock_code': '000002', 'buypoint_date': '2025-01-01'}
])

# æŠ€æœ¯æŒ‡æ ‡æŸ¥è¯¢
indicators = client.get_indicators(
    stock_code='000001',
    date='2025-01-01',
    indicators=['RSI', 'MACD', 'KDJ']
)
```

---

## æ€§èƒ½ä¼˜åŒ–åŠŸèƒ½

### âš¡ å¹¶è¡Œå¤„ç†

```python
# å¯ç”¨å¹¶è¡Œå¤„ç†
from analysis.parallel_buypoint_analyzer import ParallelBuyPointAnalyzer

analyzer = ParallelBuyPointAnalyzer(
    max_workers=8,  # 8ä¸ªå¹¶è¡Œè¿›ç¨‹
    chunk_size=10   # æ¯æ‰¹å¤„ç†10ä¸ªä¹°ç‚¹
)

# æ€§èƒ½ç›‘æ§
performance_stats = analyzer.get_performance_stats()
print(f"å¹¶è¡ŒåŠ é€Ÿæ¯”: {performance_stats['speedup_ratio']:.2f}x")
print(f"CPUåˆ©ç”¨ç‡: {performance_stats['cpu_utilization']:.1f}%")
```

### ğŸš€ å‘é‡åŒ–è®¡ç®—

```python
# å¯ç”¨å‘é‡åŒ–ä¼˜åŒ–
from analysis.vectorized_indicator_optimizer import VectorizedIndicatorOptimizer

optimizer = VectorizedIndicatorOptimizer()

# æ£€æŸ¥å‘é‡åŒ–æ”¯æŒ
vectorizable_indicators = optimizer.get_vectorizable_indicators()
print(f"æ”¯æŒå‘é‡åŒ–çš„æŒ‡æ ‡: {len(vectorizable_indicators)}ä¸ª")

# æ€§èƒ½å¯¹æ¯”æµ‹è¯•
performance_comparison = optimizer.benchmark_vectorization()
print(f"å‘é‡åŒ–æ€§èƒ½æå‡: {performance_comparison['improvement_percentage']:.1f}%")
```

### ğŸ’¾ æ™ºèƒ½ç¼“å­˜

```python
# é…ç½®æ™ºèƒ½ç¼“å­˜
from analysis.intelligent_cache_system import IntelligentCacheSystem

cache_system = IntelligentCacheSystem(
    max_memory_cache_size=1000,
    enable_disk_cache=True,
    cache_dir="data/cache"
)

# ç¼“å­˜ç»Ÿè®¡
cache_stats = cache_system.get_cache_stats()
print(f"ç¼“å­˜å‘½ä¸­ç‡: {cache_stats['hit_rate']:.1f}%")
print(f"å†…å­˜ç¼“å­˜å¤§å°: {cache_stats['memory_cache_size']}")

# æ¸…ç©ºç¼“å­˜
cache_system.clear_cache(clear_disk=True)
```

### ğŸ“Š æ€§èƒ½ç›‘æ§

```python
# æ€§èƒ½åˆ†æå™¨
from analysis.indicator_performance_profiler import IndicatorPerformanceProfiler

profiler = IndicatorPerformanceProfiler()

# è¿è¡Œæ€§èƒ½åˆ†æ
performance_report = profiler.run_comprehensive_analysis()

print("æ€§èƒ½åˆ†ææŠ¥å‘Š:")
print(f"æœ€è€—æ—¶æŒ‡æ ‡: {performance_report['slowest_indicators']}")
print(f"å¹³å‡è®¡ç®—æ—¶é—´: {performance_report['average_calculation_time']:.4f}s")
print(f"æ€§èƒ½ç“¶é¢ˆ: {performance_report['bottlenecks']}")
```

---

## å¸¸è§é—®é¢˜è§£ç­”

### â“ å®‰è£…å’Œé…ç½®é—®é¢˜

**Q: å®‰è£…ä¾èµ–æ—¶å‡ºç°é”™è¯¯æ€ä¹ˆåŠï¼Ÿ**

A: è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤æ’æŸ¥ï¼š
```bash
# 1. æ›´æ–°pip
pip install --upgrade pip

# 2. æ¸…ç†ç¼“å­˜
pip cache purge

# 3. ä½¿ç”¨å›½å†…é•œåƒæº
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple/

# 4. åˆ†æ­¥å®‰è£…æ ¸å¿ƒä¾èµ–
pip install pandas numpy scipy
pip install clickhouse-driver redis
```

**Q: æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Ÿ**

A: æ£€æŸ¥é…ç½®æ–‡ä»¶å’Œç½‘ç»œè¿æ¥ï¼š
```python
# æµ‹è¯•æ•°æ®åº“è¿æ¥
from utils.database import test_connection

connection_result = test_connection()
if not connection_result['success']:
    print(f"è¿æ¥å¤±è´¥: {connection_result['error']}")
    print("è¯·æ£€æŸ¥æ•°æ®åº“é…ç½®å’Œç½‘ç»œè¿æ¥")
```

### â“ æ€§èƒ½é—®é¢˜

**Q: ç³»ç»Ÿè¿è¡Œç¼“æ…¢æ€ä¹ˆåŠï¼Ÿ**

A: æŒ‰ä¼˜å…ˆçº§æ£€æŸ¥ä»¥ä¸‹è®¾ç½®ï¼š
```python
# 1. å¯ç”¨æ‰€æœ‰ä¼˜åŒ–é€‰é¡¹
analyzer = OptimizedBuyPointAnalyzer(
    enable_cache=True,           # å¯ç”¨ç¼“å­˜
    enable_vectorization=True    # å¯ç”¨å‘é‡åŒ–
)

# 2. è°ƒæ•´å¹¶è¡Œå¤„ç†å‚æ•°
parallel_analyzer = ParallelBuyPointAnalyzer(
    max_workers=min(8, cpu_count())  # æ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´
)

# 3. æ£€æŸ¥ç³»ç»Ÿèµ„æº
import psutil
print(f"CPUä½¿ç”¨ç‡: {psutil.cpu_percent()}%")
print(f"å†…å­˜ä½¿ç”¨ç‡: {psutil.virtual_memory().percent}%")
```

**Q: ç¼“å­˜ä¸ç”Ÿæ•ˆï¼Ÿ**

A: æ£€æŸ¥ç¼“å­˜é…ç½®å’Œæƒé™ï¼š
```python
# æ£€æŸ¥ç¼“å­˜çŠ¶æ€
cache_stats = cache_system.get_cache_stats()
if cache_stats['hit_rate'] == 0:
    print("ç¼“å­˜æœªå‘½ä¸­ï¼Œå¯èƒ½åŸå› ï¼š")
    print("1. ç¼“å­˜ç›®å½•æƒé™ä¸è¶³")
    print("2. ç£ç›˜ç©ºé—´ä¸è¶³")
    print("3. ç¼“å­˜é”®ç”Ÿæˆå¼‚å¸¸")
    
# é‡ç½®ç¼“å­˜
cache_system.clear_cache()
cache_system = IntelligentCacheSystem()  # é‡æ–°åˆå§‹åŒ–
```

### â“ æ•°æ®é—®é¢˜

**Q: æŒ‡æ ‡è®¡ç®—ç»“æœå¼‚å¸¸ï¼Ÿ**

A: è¿›è¡Œæ•°æ®éªŒè¯å’ŒæŒ‡æ ‡æ£€æŸ¥ï¼š
```python
# æ•°æ®éªŒè¯
validation_result = processor.validate_data_integrity(stock_data)
if not validation_result['is_valid']:
    print(f"æ•°æ®é—®é¢˜: {validation_result['issues']}")

# æŒ‡æ ‡è®¡ç®—éªŒè¯
indicator_validator = IndicatorValidator()
validation_report = indicator_validator.validate_all_indicators(stock_data)
print(f"æŒ‡æ ‡éªŒè¯æŠ¥å‘Š: {validation_report}")
```

---

## æ•…éšœæ’é™¤

### ğŸ”§ æ—¥å¿—åˆ†æ

```python
# å¯ç”¨è¯¦ç»†æ—¥å¿—
import logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/system.log'),
        logging.StreamHandler()
    ]
)

# æŸ¥çœ‹é”™è¯¯æ—¥å¿—
from utils.logger import get_logger
logger = get_logger(__name__)
logger.info("ç³»ç»Ÿå¯åŠ¨")
logger.error("é”™è¯¯ä¿¡æ¯", exc_info=True)
```

### ğŸ” æ€§èƒ½è¯Šæ–­

```python
# ç³»ç»Ÿæ€§èƒ½è¯Šæ–­
from analysis.system_diagnostics import SystemDiagnostics

diagnostics = SystemDiagnostics()
diagnostic_report = diagnostics.run_full_diagnosis()

print("ç³»ç»Ÿè¯Šæ–­æŠ¥å‘Š:")
print(f"CPUæ€§èƒ½: {diagnostic_report['cpu_performance']}")
print(f"å†…å­˜ä½¿ç”¨: {diagnostic_report['memory_usage']}")
print(f"ç£ç›˜IO: {diagnostic_report['disk_io']}")
print(f"æ•°æ®åº“è¿æ¥: {diagnostic_report['database_status']}")
```

### ğŸš¨ é”™è¯¯æ¢å¤

```python
# è‡ªåŠ¨é”™è¯¯æ¢å¤
from utils.error_recovery import ErrorRecoveryManager

recovery_manager = ErrorRecoveryManager()

try:
    # æ‰§è¡Œåˆ†æä»»åŠ¡
    result = analyzer.analyze_buypoint(stock_code, date)
except Exception as e:
    # è‡ªåŠ¨æ¢å¤
    recovery_result = recovery_manager.handle_error(e)
    if recovery_result['recovered']:
        print("é”™è¯¯å·²è‡ªåŠ¨æ¢å¤")
        result = recovery_result['result']
    else:
        print(f"æ¢å¤å¤±è´¥: {recovery_result['error']}")
```

---

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚æœé‡åˆ°æ— æ³•è§£å†³çš„é—®é¢˜ï¼Œè¯·è”ç³»æŠ€æœ¯æ”¯æŒï¼š

- **é‚®ç®±**: support@stockanalysis.com
- **æ–‡æ¡£**: https://docs.stockanalysis.com
- **GitHub Issues**: https://github.com/your-repo/issues
- **æŠ€æœ¯è®ºå›**: https://forum.stockanalysis.com

---

*æ–‡æ¡£ç‰ˆæœ¬: v2.0*  
*æœ€åæ›´æ–°: 2025-06-15*  
*é€‚ç”¨ç³»ç»Ÿç‰ˆæœ¬: v2.0+*
