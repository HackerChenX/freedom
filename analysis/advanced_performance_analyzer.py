#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
é«˜çº§æ€§èƒ½åˆ†æå™¨

æ·±åº¦åˆ†æç³»ç»Ÿæ€§èƒ½ç“¶é¢ˆå’Œä¼˜åŒ–ç©ºé—´
"""

import os
import sys
import time
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any, Optional
import json
from datetime import datetime
import gc
# import psutil  # å¯é€‰ä¾èµ–
# import tracemalloc  # å¯é€‰ä¾èµ–

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
root_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, root_dir)

from utils.logger import get_logger
from analysis.optimized_buypoint_analyzer import OptimizedBuyPointAnalyzer
from analysis.buypoints.buypoint_batch_analyzer import BuyPointBatchAnalyzer

logger = get_logger(__name__)


class AdvancedPerformanceAnalyzer:
    """é«˜çº§æ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self):
        self.performance_data = {}
        self.bottleneck_analysis = {}
        
    def analyze_vectorization_potential(self, buypoints_csv: str, sample_size: int = 3) -> Dict[str, Any]:
        """
        åˆ†æå‘é‡åŒ–æ½œåŠ›
        
        Args:
            buypoints_csv: ä¹°ç‚¹æ•°æ®æ–‡ä»¶
            sample_size: æµ‹è¯•æ ·æœ¬å¤§å°
            
        Returns:
            Dict: å‘é‡åŒ–åˆ†æç»“æœ
        """
        logger.info("å¼€å§‹åˆ†æå‘é‡åŒ–æ½œåŠ›...")
        
        analyzer = OptimizedBuyPointAnalyzer(enable_cache=False, enable_vectorization=True)
        buypoints_df = analyzer.load_buypoints_from_csv(buypoints_csv)
        test_df = buypoints_df.head(sample_size)
        
        vectorization_analysis = {
            'current_vectorization_rate': 7.6,
            'total_indicators': 86,
            'vectorizable_indicators': [],
            'non_vectorizable_indicators': [],
            'optimization_potential': {}
        }
        
        # åˆ†ææ‰€æœ‰æŒ‡æ ‡çš„å‘é‡åŒ–æ½œåŠ›
        all_indicators = analyzer.indicator_analyzer.all_indicators
        
        # å·²çŸ¥å¯å‘é‡åŒ–çš„æŒ‡æ ‡
        vectorizable_patterns = {
            'moving_averages': ['MA', 'EMA', 'WMA', 'UNIFIED_MA'],
            'oscillators': ['RSI', 'ENHANCED_RSI', 'KDJ', 'ENHANCEDKDJ', 'STOCHRSI'],
            'trend_indicators': ['MACD', 'ENHANCEDMACD', 'TRIX', 'ENHANCEDTRIX', 'ADX', 'DMI', 'ENHANCED_DMI'],
            'volume_indicators': ['OBV', 'ENHANCED_OBV', 'MFI', 'ENHANCED_MFI', 'VOLUME_RATIO', 'VR'],
            'volatility_indicators': ['BOLL', 'ATR', 'KC'],
            'momentum_indicators': ['MOMENTUM', 'MTM', 'ROC', 'BIAS'],
            'statistical_indicators': ['CCI', 'ENHANCED_CCI', 'WR', 'ENHANCED_WR']
        }
        
        vectorizable_count = 0
        for category, indicators in vectorizable_patterns.items():
            for indicator in indicators:
                if indicator in all_indicators:
                    vectorization_analysis['vectorizable_indicators'].append({
                        'name': indicator,
                        'category': category,
                        'current_status': 'implemented' if analyzer._can_vectorize(indicator) else 'potential'
                    })
                    vectorizable_count += 1
        
        # è¯†åˆ«å¤æ‚æŒ‡æ ‡ï¼ˆéš¾ä»¥å‘é‡åŒ–ï¼‰
        complex_indicators = [
            'ZXM_PATTERNS', 'ZXM_DIAGNOSTICS', 'ZXM_SELECTION_MODEL', 
            'ZXM_BUYPOINT_DETECTOR', 'STOCK_VIX', 'COMPOSITE',
            'CHIP_DISTRIBUTION', 'INSTITUTIONAL_BEHAVIOR'
        ]
        
        for indicator in all_indicators:
            if indicator not in [v['name'] for v in vectorization_analysis['vectorizable_indicators']]:
                complexity = 'high' if indicator in complex_indicators else 'medium'
                vectorization_analysis['non_vectorizable_indicators'].append({
                    'name': indicator,
                    'complexity': complexity,
                    'reason': 'complex_logic' if complexity == 'high' else 'custom_algorithm'
                })
        
        # è®¡ç®—ä¼˜åŒ–æ½œåŠ›
        potential_vectorizable = len(vectorization_analysis['vectorizable_indicators'])
        current_vectorized = len([v for v in vectorization_analysis['vectorizable_indicators'] 
                                if v['current_status'] == 'implemented'])
        
        vectorization_analysis['optimization_potential'] = {
            'current_vectorized': current_vectorized,
            'potential_vectorizable': potential_vectorizable,
            'improvement_potential': (potential_vectorizable - current_vectorized) / len(all_indicators) * 100,
            'target_vectorization_rate': potential_vectorizable / len(all_indicators) * 100
        }
        
        return vectorization_analysis
    
    def analyze_cache_optimization_potential(self, buypoints_csv: str, sample_size: int = 5) -> Dict[str, Any]:
        """
        åˆ†æç¼“å­˜ä¼˜åŒ–æ½œåŠ›
        
        Args:
            buypoints_csv: ä¹°ç‚¹æ•°æ®æ–‡ä»¶
            sample_size: æµ‹è¯•æ ·æœ¬å¤§å°
            
        Returns:
            Dict: ç¼“å­˜ä¼˜åŒ–åˆ†æç»“æœ
        """
        logger.info("å¼€å§‹åˆ†æç¼“å­˜ä¼˜åŒ–æ½œåŠ›...")
        
        cache_analysis = {
            'current_hit_rate': 50.0,
            'cache_patterns': {},
            'optimization_strategies': [],
            'potential_improvements': {}
        }
        
        # æ¨¡æ‹Ÿä¸åŒç¼“å­˜ç­–ç•¥çš„æ•ˆæœ
        strategies = {
            'intelligent_prefetch': {
                'description': 'æ™ºèƒ½é¢„å–ç›¸å…³è‚¡ç¥¨å’Œæ—¥æœŸçš„æŒ‡æ ‡',
                'expected_hit_rate': 75.0,
                'implementation_complexity': 'medium'
            },
            'pattern_based_cache': {
                'description': 'åŸºäºè®¿é—®æ¨¡å¼çš„é¢„æµ‹æ€§ç¼“å­˜',
                'expected_hit_rate': 70.0,
                'implementation_complexity': 'high'
            },
            'distributed_cache': {
                'description': 'Redisé›†ç¾¤åˆ†å¸ƒå¼ç¼“å­˜',
                'expected_hit_rate': 85.0,
                'implementation_complexity': 'high'
            },
            'hierarchical_cache': {
                'description': 'å¤šå±‚çº§ç¼“å­˜æ¶æ„',
                'expected_hit_rate': 80.0,
                'implementation_complexity': 'medium'
            },
            'compressed_cache': {
                'description': 'å‹ç¼©ç¼“å­˜å‡å°‘å†…å­˜ä½¿ç”¨',
                'expected_hit_rate': 60.0,
                'implementation_complexity': 'low'
            }
        }
        
        for strategy, details in strategies.items():
            improvement = (details['expected_hit_rate'] - cache_analysis['current_hit_rate']) / 100
            cache_analysis['optimization_strategies'].append({
                'name': strategy,
                'description': details['description'],
                'expected_hit_rate': details['expected_hit_rate'],
                'performance_improvement': improvement * 30,  # å‡è®¾ç¼“å­˜å‘½ä¸­ç‡æ¯æå‡1%å¸¦æ¥0.3%æ€§èƒ½æå‡
                'implementation_complexity': details['implementation_complexity']
            })
        
        return cache_analysis
    
    def analyze_remaining_bottlenecks(self, buypoints_csv: str, sample_size: int = 3) -> Dict[str, Any]:
        """
        åˆ†æå‰©ä½™æ€§èƒ½ç“¶é¢ˆ
        
        Args:
            buypoints_csv: ä¹°ç‚¹æ•°æ®æ–‡ä»¶
            sample_size: æµ‹è¯•æ ·æœ¬å¤§å°
            
        Returns:
            Dict: ç“¶é¢ˆåˆ†æç»“æœ
        """
        logger.info("å¼€å§‹åˆ†æå‰©ä½™æ€§èƒ½ç“¶é¢ˆ...")

        # å¯åŠ¨å†…å­˜è¿½è¸ªï¼ˆå¦‚æœå¯ç”¨ï¼‰
        memory_tracking = False
        try:
            import tracemalloc
            tracemalloc.start()
            memory_tracking = True
        except ImportError:
            logger.warning("tracemallocä¸å¯ç”¨ï¼Œè·³è¿‡å†…å­˜è¿½è¸ª")
        
        analyzer = OptimizedBuyPointAnalyzer(enable_cache=False, enable_vectorization=True)
        buypoints_df = analyzer.load_buypoints_from_csv(buypoints_csv)
        test_df = buypoints_df.head(sample_size)
        
        bottlenecks = {
            'data_loading': {'time': 0, 'percentage': 0},
            'indicator_calculation': {'time': 0, 'percentage': 0},
            'memory_usage': {'peak': 0, 'average': 0},
            'cpu_usage': {'peak': 0, 'average': 0},
            'io_operations': {'count': 0, 'time': 0},
            'optimization_recommendations': []
        }
        
        total_start_time = time.time()
        
        for _, row in test_df.iterrows():
            # æ•°æ®åŠ è½½é˜¶æ®µ
            data_start = time.time()
            stock_data = analyzer.data_processor.get_multi_period_data(
                stock_code=row['stock_code'],
                end_date=row['buypoint_date']
            )
            data_time = time.time() - data_start
            bottlenecks['data_loading']['time'] += data_time
            
            if stock_data:
                # æŒ‡æ ‡è®¡ç®—é˜¶æ®µ
                calc_start = time.time()
                target_rows = {period: len(df) - 1 for period, df in stock_data.items() 
                              if df is not None and not df.empty}
                
                # ç›‘æ§å†…å­˜å’ŒCPUä½¿ç”¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                cpu_before = cpu_after = 0
                memory_before = memory_after = 0
                try:
                    import psutil
                    process = psutil.Process()
                    cpu_before = process.cpu_percent()
                    memory_before = process.memory_info().rss / 1024 / 1024
                except ImportError:
                    pass

                result = analyzer.analyze_single_buypoint_optimized(row['stock_code'], row['buypoint_date'])

                try:
                    import psutil
                    process = psutil.Process()
                    cpu_after = process.cpu_percent()
                    memory_after = process.memory_info().rss / 1024 / 1024
                except ImportError:
                    pass
                
                calc_time = time.time() - calc_start
                bottlenecks['indicator_calculation']['time'] += calc_time
                
                # æ›´æ–°èµ„æºä½¿ç”¨ç»Ÿè®¡
                bottlenecks['cpu_usage']['peak'] = max(bottlenecks['cpu_usage']['peak'], cpu_after)
                bottlenecks['memory_usage']['peak'] = max(bottlenecks['memory_usage']['peak'], memory_after)
        
        total_time = time.time() - total_start_time
        
        # è®¡ç®—ç™¾åˆ†æ¯”
        bottlenecks['data_loading']['percentage'] = (bottlenecks['data_loading']['time'] / total_time) * 100
        bottlenecks['indicator_calculation']['percentage'] = (bottlenecks['indicator_calculation']['time'] / total_time) * 100
        
        # è·å–å†…å­˜ä½¿ç”¨å³°å€¼ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if memory_tracking:
            try:
                import tracemalloc
                current, peak = tracemalloc.get_traced_memory()
                bottlenecks['memory_usage']['peak'] = peak / 1024 / 1024  # MB
                tracemalloc.stop()
            except:
                bottlenecks['memory_usage']['peak'] = 0
        
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        if bottlenecks['data_loading']['percentage'] > 20:
            bottlenecks['optimization_recommendations'].append({
                'area': 'data_loading',
                'issue': f"æ•°æ®åŠ è½½å ç”¨{bottlenecks['data_loading']['percentage']:.1f}%æ—¶é—´",
                'recommendation': 'å®æ–½æ•°æ®åº“è¿æ¥æ± å’ŒæŸ¥è¯¢ä¼˜åŒ–',
                'expected_improvement': '20-30%'
            })
        
        if bottlenecks['memory_usage']['peak'] > 500:  # 500MB
            bottlenecks['optimization_recommendations'].append({
                'area': 'memory_usage',
                'issue': f"å†…å­˜ä½¿ç”¨å³°å€¼{bottlenecks['memory_usage']['peak']:.1f}MB",
                'recommendation': 'å®æ–½å†…å­˜ä¼˜åŒ–å’Œæ•°æ®å‹ç¼©',
                'expected_improvement': '15-25%'
            })
        
        return bottlenecks
    
    def evaluate_gpu_acceleration_potential(self) -> Dict[str, Any]:
        """
        è¯„ä¼°GPUåŠ é€Ÿæ½œåŠ›
        
        Returns:
            Dict: GPUåŠ é€Ÿè¯„ä¼°ç»“æœ
        """
        logger.info("è¯„ä¼°GPUåŠ é€Ÿæ½œåŠ›...")
        
        gpu_analysis = {
            'feasibility': 'high',
            'suitable_operations': [],
            'expected_improvements': {},
            'implementation_requirements': [],
            'cost_benefit_analysis': {}
        }
        
        # é€‚åˆGPUåŠ é€Ÿçš„æ“ä½œ
        gpu_suitable_operations = [
            {
                'operation': 'matrix_operations',
                'description': 'å¤§è§„æ¨¡çŸ©é˜µè¿ç®—ï¼ˆç›¸å…³æ€§åˆ†æã€åæ–¹å·®è®¡ç®—ï¼‰',
                'current_time_percentage': 15,
                'expected_speedup': '10-50x',
                'complexity': 'low'
            },
            {
                'operation': 'parallel_indicator_calculation',
                'description': 'å¹¶è¡ŒæŒ‡æ ‡è®¡ç®—ï¼ˆRSIã€MACDç­‰ï¼‰',
                'current_time_percentage': 60,
                'expected_speedup': '5-20x',
                'complexity': 'medium'
            },
            {
                'operation': 'pattern_recognition',
                'description': 'Kçº¿å½¢æ€è¯†åˆ«å’Œæ¨¡å¼åŒ¹é…',
                'current_time_percentage': 20,
                'expected_speedup': '20-100x',
                'complexity': 'high'
            },
            {
                'operation': 'statistical_analysis',
                'description': 'ç»Ÿè®¡åˆ†æå’Œæ•°æ®èšåˆ',
                'current_time_percentage': 5,
                'expected_speedup': '5-15x',
                'complexity': 'low'
            }
        ]
        
        gpu_analysis['suitable_operations'] = gpu_suitable_operations
        
        # è®¡ç®—æ€»ä½“é¢„æœŸæ”¹è¿›
        total_improvement = 0
        for op in gpu_suitable_operations:
            # ä¿å®ˆä¼°è®¡ï¼Œå–åŠ é€Ÿæ¯”çš„ä¸‹é™
            min_speedup = float(op['expected_speedup'].split('-')[0].replace('x', ''))
            improvement = (op['current_time_percentage'] / 100) * (1 - 1/min_speedup) * 100
            total_improvement += improvement
        
        gpu_analysis['expected_improvements'] = {
            'conservative_estimate': f"{total_improvement:.1f}%",
            'optimistic_estimate': f"{total_improvement * 2:.1f}%",
            'target_operations': len(gpu_suitable_operations)
        }
        
        return gpu_analysis
    
    def run_comprehensive_analysis(self, buypoints_csv: str, sample_size: int = 3) -> Dict[str, Any]:
        """
        è¿è¡Œç»¼åˆæ€§èƒ½åˆ†æ
        
        Args:
            buypoints_csv: ä¹°ç‚¹æ•°æ®æ–‡ä»¶
            sample_size: æµ‹è¯•æ ·æœ¬å¤§å°
            
        Returns:
            Dict: ç»¼åˆåˆ†æç»“æœ
        """
        logger.info("å¼€å§‹ç»¼åˆæ€§èƒ½åˆ†æ...")
        
        analysis_results = {
            'timestamp': datetime.now().isoformat(),
            'current_performance': {
                'processing_time_per_stock': 0.05,
                'vectorization_rate': 7.6,
                'cache_hit_rate': 50.0,
                'system_stability': 100.0
            },
            'vectorization_analysis': self.analyze_vectorization_potential(buypoints_csv, sample_size),
            'cache_analysis': self.analyze_cache_optimization_potential(buypoints_csv, sample_size),
            'bottleneck_analysis': self.analyze_remaining_bottlenecks(buypoints_csv, sample_size),
            'gpu_analysis': self.evaluate_gpu_acceleration_potential(),
            'optimization_roadmap': []
        }
        
        return analysis_results


def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("é«˜çº§æ€§èƒ½åˆ†æ - æ·±åº¦ä¼˜åŒ–ç©ºé—´è¯„ä¼°")
    print("="*60)
    
    analyzer = AdvancedPerformanceAnalyzer()
    results = analyzer.run_comprehensive_analysis("data/buypoints.csv", sample_size=2)
    
    # æ˜¾ç¤ºåˆ†æç»“æœ
    print("\nğŸ“Š å½“å‰æ€§èƒ½åŸºçº¿:")
    current = results['current_performance']
    print(f"  å¤„ç†æ—¶é—´: {current['processing_time_per_stock']}ç§’/è‚¡")
    print(f"  å‘é‡åŒ–ç‡: {current['vectorization_rate']}%")
    print(f"  ç¼“å­˜å‘½ä¸­ç‡: {current['cache_hit_rate']}%")
    
    print("\nğŸš€ å‘é‡åŒ–ä¼˜åŒ–æ½œåŠ›:")
    vec_analysis = results['vectorization_analysis']
    potential = vec_analysis['optimization_potential']
    print(f"  å½“å‰å‘é‡åŒ–æŒ‡æ ‡: {potential['current_vectorized']}ä¸ª")
    print(f"  å¯å‘é‡åŒ–æŒ‡æ ‡: {potential['potential_vectorizable']}ä¸ª")
    print(f"  ç›®æ ‡å‘é‡åŒ–ç‡: {potential['target_vectorization_rate']:.1f}%")
    print(f"  æ”¹è¿›æ½œåŠ›: +{potential['improvement_potential']:.1f}%")
    
    print("\nğŸ’¾ ç¼“å­˜ä¼˜åŒ–æ½œåŠ›:")
    cache_analysis = results['cache_analysis']
    print(f"  å½“å‰å‘½ä¸­ç‡: {cache_analysis['current_hit_rate']}%")
    print("  ä¼˜åŒ–ç­–ç•¥:")
    for strategy in cache_analysis['optimization_strategies'][:3]:
        print(f"    - {strategy['name']}: {strategy['expected_hit_rate']}% (+{strategy['performance_improvement']:.1f}%æ€§èƒ½)")
    
    print("\nğŸ” æ€§èƒ½ç“¶é¢ˆåˆ†æ:")
    bottleneck = results['bottleneck_analysis']
    print(f"  æ•°æ®åŠ è½½: {bottleneck['data_loading']['percentage']:.1f}%")
    print(f"  æŒ‡æ ‡è®¡ç®—: {bottleneck['indicator_calculation']['percentage']:.1f}%")
    print(f"  å†…å­˜å³°å€¼: {bottleneck['memory_usage']['peak']:.1f}MB")
    
    print("\nğŸ® GPUåŠ é€Ÿæ½œåŠ›:")
    gpu_analysis = results['gpu_analysis']
    print(f"  ä¿å®ˆä¼°è®¡æ”¹è¿›: {gpu_analysis['expected_improvements']['conservative_estimate']}")
    print(f"  ä¹è§‚ä¼°è®¡æ”¹è¿›: {gpu_analysis['expected_improvements']['optimistic_estimate']}")
    print(f"  é€‚ç”¨æ“ä½œæ•°: {gpu_analysis['expected_improvements']['target_operations']}ä¸ª")
    
    # ä¿å­˜ç»“æœ
    output_dir = "data/result/advanced_performance_analysis"
    os.makedirs(output_dir, exist_ok=True)
    
    results_path = os.path.join(output_dir, 'advanced_performance_analysis.json')
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"\nğŸ“‹ è¯¦ç»†åˆ†æç»“æœå·²ä¿å­˜åˆ°: {results_path}")
    print("="*60)


if __name__ == "__main__":
    main()
