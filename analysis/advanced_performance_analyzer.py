#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
高级性能分析器

深度分析系统性能瓶颈和优化空间
"""

import os
import sys
import time
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any, Optional
import json
from datetime import datetime
import gc
# import psutil  # 可选依赖
# import tracemalloc  # 可选依赖

# 添加项目根目录到Python路径
root_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, root_dir)

from utils.logger import get_logger
from analysis.optimized_buypoint_analyzer import OptimizedBuyPointAnalyzer
from analysis.buypoints.buypoint_batch_analyzer import BuyPointBatchAnalyzer

logger = get_logger(__name__)


class AdvancedPerformanceAnalyzer:
    """高级性能分析器"""
    
    def __init__(self):
        self.performance_data = {}
        self.bottleneck_analysis = {}
        
    def analyze_vectorization_potential(self, buypoints_csv: str, sample_size: int = 3) -> Dict[str, Any]:
        """
        分析向量化潜力
        
        Args:
            buypoints_csv: 买点数据文件
            sample_size: 测试样本大小
            
        Returns:
            Dict: 向量化分析结果
        """
        logger.info("开始分析向量化潜力...")
        
        analyzer = OptimizedBuyPointAnalyzer(enable_cache=False, enable_vectorization=True)
        buypoints_df = analyzer.load_buypoints_from_csv(buypoints_csv)
        test_df = buypoints_df.head(sample_size)
        
        vectorization_analysis = {
            'current_vectorization_rate': 7.6,
            'total_indicators': 86,
            'vectorizable_indicators': [],
            'non_vectorizable_indicators': [],
            'optimization_potential': {}
        }
        
        # 分析所有指标的向量化潜力
        all_indicators = analyzer.indicator_analyzer.all_indicators
        
        # 已知可向量化的指标
        vectorizable_patterns = {
            'moving_averages': ['MA', 'EMA', 'WMA', 'UNIFIED_MA'],
            'oscillators': ['RSI', 'ENHANCED_RSI', 'KDJ', 'ENHANCEDKDJ', 'STOCHRSI'],
            'trend_indicators': ['MACD', 'ENHANCEDMACD', 'TRIX', 'ENHANCEDTRIX', 'ADX', 'DMI', 'ENHANCED_DMI'],
            'volume_indicators': ['OBV', 'ENHANCED_OBV', 'MFI', 'ENHANCED_MFI', 'VOLUME_RATIO', 'VR'],
            'volatility_indicators': ['BOLL', 'ATR', 'KC'],
            'momentum_indicators': ['MOMENTUM', 'MTM', 'ROC', 'BIAS'],
            'statistical_indicators': ['CCI', 'ENHANCED_CCI', 'WR', 'ENHANCED_WR']
        }
        
        vectorizable_count = 0
        for category, indicators in vectorizable_patterns.items():
            for indicator in indicators:
                if indicator in all_indicators:
                    vectorization_analysis['vectorizable_indicators'].append({
                        'name': indicator,
                        'category': category,
                        'current_status': 'implemented' if analyzer._can_vectorize(indicator) else 'potential'
                    })
                    vectorizable_count += 1
        
        # 识别复杂指标（难以向量化）
        complex_indicators = [
            'ZXM_PATTERNS', 'ZXM_DIAGNOSTICS', 'ZXM_SELECTION_MODEL', 
            'ZXM_BUYPOINT_DETECTOR', 'STOCK_VIX', 'COMPOSITE',
            'CHIP_DISTRIBUTION', 'INSTITUTIONAL_BEHAVIOR'
        ]
        
        for indicator in all_indicators:
            if indicator not in [v['name'] for v in vectorization_analysis['vectorizable_indicators']]:
                complexity = 'high' if indicator in complex_indicators else 'medium'
                vectorization_analysis['non_vectorizable_indicators'].append({
                    'name': indicator,
                    'complexity': complexity,
                    'reason': 'complex_logic' if complexity == 'high' else 'custom_algorithm'
                })
        
        # 计算优化潜力
        potential_vectorizable = len(vectorization_analysis['vectorizable_indicators'])
        current_vectorized = len([v for v in vectorization_analysis['vectorizable_indicators'] 
                                if v['current_status'] == 'implemented'])
        
        vectorization_analysis['optimization_potential'] = {
            'current_vectorized': current_vectorized,
            'potential_vectorizable': potential_vectorizable,
            'improvement_potential': (potential_vectorizable - current_vectorized) / len(all_indicators) * 100,
            'target_vectorization_rate': potential_vectorizable / len(all_indicators) * 100
        }
        
        return vectorization_analysis
    
    def analyze_cache_optimization_potential(self, buypoints_csv: str, sample_size: int = 5) -> Dict[str, Any]:
        """
        分析缓存优化潜力
        
        Args:
            buypoints_csv: 买点数据文件
            sample_size: 测试样本大小
            
        Returns:
            Dict: 缓存优化分析结果
        """
        logger.info("开始分析缓存优化潜力...")
        
        cache_analysis = {
            'current_hit_rate': 50.0,
            'cache_patterns': {},
            'optimization_strategies': [],
            'potential_improvements': {}
        }
        
        # 模拟不同缓存策略的效果
        strategies = {
            'intelligent_prefetch': {
                'description': '智能预取相关股票和日期的指标',
                'expected_hit_rate': 75.0,
                'implementation_complexity': 'medium'
            },
            'pattern_based_cache': {
                'description': '基于访问模式的预测性缓存',
                'expected_hit_rate': 70.0,
                'implementation_complexity': 'high'
            },
            'distributed_cache': {
                'description': 'Redis集群分布式缓存',
                'expected_hit_rate': 85.0,
                'implementation_complexity': 'high'
            },
            'hierarchical_cache': {
                'description': '多层级缓存架构',
                'expected_hit_rate': 80.0,
                'implementation_complexity': 'medium'
            },
            'compressed_cache': {
                'description': '压缩缓存减少内存使用',
                'expected_hit_rate': 60.0,
                'implementation_complexity': 'low'
            }
        }
        
        for strategy, details in strategies.items():
            improvement = (details['expected_hit_rate'] - cache_analysis['current_hit_rate']) / 100
            cache_analysis['optimization_strategies'].append({
                'name': strategy,
                'description': details['description'],
                'expected_hit_rate': details['expected_hit_rate'],
                'performance_improvement': improvement * 30,  # 假设缓存命中率每提升1%带来0.3%性能提升
                'implementation_complexity': details['implementation_complexity']
            })
        
        return cache_analysis
    
    def analyze_remaining_bottlenecks(self, buypoints_csv: str, sample_size: int = 3) -> Dict[str, Any]:
        """
        分析剩余性能瓶颈
        
        Args:
            buypoints_csv: 买点数据文件
            sample_size: 测试样本大小
            
        Returns:
            Dict: 瓶颈分析结果
        """
        logger.info("开始分析剩余性能瓶颈...")

        # 启动内存追踪（如果可用）
        memory_tracking = False
        try:
            import tracemalloc
            tracemalloc.start()
            memory_tracking = True
        except ImportError:
            logger.warning("tracemalloc不可用，跳过内存追踪")
        
        analyzer = OptimizedBuyPointAnalyzer(enable_cache=False, enable_vectorization=True)
        buypoints_df = analyzer.load_buypoints_from_csv(buypoints_csv)
        test_df = buypoints_df.head(sample_size)
        
        bottlenecks = {
            'data_loading': {'time': 0, 'percentage': 0},
            'indicator_calculation': {'time': 0, 'percentage': 0},
            'memory_usage': {'peak': 0, 'average': 0},
            'cpu_usage': {'peak': 0, 'average': 0},
            'io_operations': {'count': 0, 'time': 0},
            'optimization_recommendations': []
        }
        
        total_start_time = time.time()
        
        for _, row in test_df.iterrows():
            # 数据加载阶段
            data_start = time.time()
            stock_data = analyzer.data_processor.get_multi_period_data(
                stock_code=row['stock_code'],
                end_date=row['buypoint_date']
            )
            data_time = time.time() - data_start
            bottlenecks['data_loading']['time'] += data_time
            
            if stock_data:
                # 指标计算阶段
                calc_start = time.time()
                target_rows = {period: len(df) - 1 for period, df in stock_data.items() 
                              if df is not None and not df.empty}
                
                # 监控内存和CPU使用（如果可用）
                cpu_before = cpu_after = 0
                memory_before = memory_after = 0
                try:
                    import psutil
                    process = psutil.Process()
                    cpu_before = process.cpu_percent()
                    memory_before = process.memory_info().rss / 1024 / 1024
                except ImportError:
                    pass

                result = analyzer.analyze_single_buypoint_optimized(row['stock_code'], row['buypoint_date'])

                try:
                    import psutil
                    process = psutil.Process()
                    cpu_after = process.cpu_percent()
                    memory_after = process.memory_info().rss / 1024 / 1024
                except ImportError:
                    pass
                
                calc_time = time.time() - calc_start
                bottlenecks['indicator_calculation']['time'] += calc_time
                
                # 更新资源使用统计
                bottlenecks['cpu_usage']['peak'] = max(bottlenecks['cpu_usage']['peak'], cpu_after)
                bottlenecks['memory_usage']['peak'] = max(bottlenecks['memory_usage']['peak'], memory_after)
        
        total_time = time.time() - total_start_time
        
        # 计算百分比
        bottlenecks['data_loading']['percentage'] = (bottlenecks['data_loading']['time'] / total_time) * 100
        bottlenecks['indicator_calculation']['percentage'] = (bottlenecks['indicator_calculation']['time'] / total_time) * 100
        
        # 获取内存使用峰值（如果可用）
        if memory_tracking:
            try:
                import tracemalloc
                current, peak = tracemalloc.get_traced_memory()
                bottlenecks['memory_usage']['peak'] = peak / 1024 / 1024  # MB
                tracemalloc.stop()
            except:
                bottlenecks['memory_usage']['peak'] = 0
        
        # 生成优化建议
        if bottlenecks['data_loading']['percentage'] > 20:
            bottlenecks['optimization_recommendations'].append({
                'area': 'data_loading',
                'issue': f"数据加载占用{bottlenecks['data_loading']['percentage']:.1f}%时间",
                'recommendation': '实施数据库连接池和查询优化',
                'expected_improvement': '20-30%'
            })
        
        if bottlenecks['memory_usage']['peak'] > 500:  # 500MB
            bottlenecks['optimization_recommendations'].append({
                'area': 'memory_usage',
                'issue': f"内存使用峰值{bottlenecks['memory_usage']['peak']:.1f}MB",
                'recommendation': '实施内存优化和数据压缩',
                'expected_improvement': '15-25%'
            })
        
        return bottlenecks
    
    def evaluate_gpu_acceleration_potential(self) -> Dict[str, Any]:
        """
        评估GPU加速潜力
        
        Returns:
            Dict: GPU加速评估结果
        """
        logger.info("评估GPU加速潜力...")
        
        gpu_analysis = {
            'feasibility': 'high',
            'suitable_operations': [],
            'expected_improvements': {},
            'implementation_requirements': [],
            'cost_benefit_analysis': {}
        }
        
        # 适合GPU加速的操作
        gpu_suitable_operations = [
            {
                'operation': 'matrix_operations',
                'description': '大规模矩阵运算（相关性分析、协方差计算）',
                'current_time_percentage': 15,
                'expected_speedup': '10-50x',
                'complexity': 'low'
            },
            {
                'operation': 'parallel_indicator_calculation',
                'description': '并行指标计算（RSI、MACD等）',
                'current_time_percentage': 60,
                'expected_speedup': '5-20x',
                'complexity': 'medium'
            },
            {
                'operation': 'pattern_recognition',
                'description': 'K线形态识别和模式匹配',
                'current_time_percentage': 20,
                'expected_speedup': '20-100x',
                'complexity': 'high'
            },
            {
                'operation': 'statistical_analysis',
                'description': '统计分析和数据聚合',
                'current_time_percentage': 5,
                'expected_speedup': '5-15x',
                'complexity': 'low'
            }
        ]
        
        gpu_analysis['suitable_operations'] = gpu_suitable_operations
        
        # 计算总体预期改进
        total_improvement = 0
        for op in gpu_suitable_operations:
            # 保守估计，取加速比的下限
            min_speedup = float(op['expected_speedup'].split('-')[0].replace('x', ''))
            improvement = (op['current_time_percentage'] / 100) * (1 - 1/min_speedup) * 100
            total_improvement += improvement
        
        gpu_analysis['expected_improvements'] = {
            'conservative_estimate': f"{total_improvement:.1f}%",
            'optimistic_estimate': f"{total_improvement * 2:.1f}%",
            'target_operations': len(gpu_suitable_operations)
        }
        
        return gpu_analysis
    
    def run_comprehensive_analysis(self, buypoints_csv: str, sample_size: int = 3) -> Dict[str, Any]:
        """
        运行综合性能分析
        
        Args:
            buypoints_csv: 买点数据文件
            sample_size: 测试样本大小
            
        Returns:
            Dict: 综合分析结果
        """
        logger.info("开始综合性能分析...")
        
        analysis_results = {
            'timestamp': datetime.now().isoformat(),
            'current_performance': {
                'processing_time_per_stock': 0.05,
                'vectorization_rate': 7.6,
                'cache_hit_rate': 50.0,
                'system_stability': 100.0
            },
            'vectorization_analysis': self.analyze_vectorization_potential(buypoints_csv, sample_size),
            'cache_analysis': self.analyze_cache_optimization_potential(buypoints_csv, sample_size),
            'bottleneck_analysis': self.analyze_remaining_bottlenecks(buypoints_csv, sample_size),
            'gpu_analysis': self.evaluate_gpu_acceleration_potential(),
            'optimization_roadmap': []
        }
        
        return analysis_results


def main():
    """主函数"""
    print("="*60)
    print("高级性能分析 - 深度优化空间评估")
    print("="*60)
    
    analyzer = AdvancedPerformanceAnalyzer()
    results = analyzer.run_comprehensive_analysis("data/buypoints.csv", sample_size=2)
    
    # 显示分析结果
    print("\n📊 当前性能基线:")
    current = results['current_performance']
    print(f"  处理时间: {current['processing_time_per_stock']}秒/股")
    print(f"  向量化率: {current['vectorization_rate']}%")
    print(f"  缓存命中率: {current['cache_hit_rate']}%")
    
    print("\n🚀 向量化优化潜力:")
    vec_analysis = results['vectorization_analysis']
    potential = vec_analysis['optimization_potential']
    print(f"  当前向量化指标: {potential['current_vectorized']}个")
    print(f"  可向量化指标: {potential['potential_vectorizable']}个")
    print(f"  目标向量化率: {potential['target_vectorization_rate']:.1f}%")
    print(f"  改进潜力: +{potential['improvement_potential']:.1f}%")
    
    print("\n💾 缓存优化潜力:")
    cache_analysis = results['cache_analysis']
    print(f"  当前命中率: {cache_analysis['current_hit_rate']}%")
    print("  优化策略:")
    for strategy in cache_analysis['optimization_strategies'][:3]:
        print(f"    - {strategy['name']}: {strategy['expected_hit_rate']}% (+{strategy['performance_improvement']:.1f}%性能)")
    
    print("\n🔍 性能瓶颈分析:")
    bottleneck = results['bottleneck_analysis']
    print(f"  数据加载: {bottleneck['data_loading']['percentage']:.1f}%")
    print(f"  指标计算: {bottleneck['indicator_calculation']['percentage']:.1f}%")
    print(f"  内存峰值: {bottleneck['memory_usage']['peak']:.1f}MB")
    
    print("\n🎮 GPU加速潜力:")
    gpu_analysis = results['gpu_analysis']
    print(f"  保守估计改进: {gpu_analysis['expected_improvements']['conservative_estimate']}")
    print(f"  乐观估计改进: {gpu_analysis['expected_improvements']['optimistic_estimate']}")
    print(f"  适用操作数: {gpu_analysis['expected_improvements']['target_operations']}个")
    
    # 保存结果
    output_dir = "data/result/advanced_performance_analysis"
    os.makedirs(output_dir, exist_ok=True)
    
    results_path = os.path.join(output_dir, 'advanced_performance_analysis.json')
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"\n📋 详细分析结果已保存到: {results_path}")
    print("="*60)


if __name__ == "__main__":
    main()
